{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get the data path\n",
    "data_path = Path.cwd().parent.resolve()/\"data\"/\"GFSISB\"/\"GFSIBS2015\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"GFSIBS2015_01-16-2025.csv\"\n",
    "\n",
    "# Combine the base path and file name\n",
    "file_path = data_path / file_name\n",
    "\n",
    "# Read the file in chunks\n",
    "chunksize = 10_000\n",
    "\n",
    "if file_path.exists():  # Check if the file exists\n",
    "    data_chunks = pd.read_csv(file_path, chunksize=chunksize)\n",
    "    data = pd.concat(data_chunks, ignore_index=True)\n",
    "    print(\"File loaded successfully.\")\n",
    "else:\n",
    "    print(f\"File not found at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       Country Name  Country Code  \\\n",
      "0  Afghanistan, Islamic Republic of           512   \n",
      "1  Afghanistan, Islamic Republic of           512   \n",
      "2  Afghanistan, Islamic Republic of           512   \n",
      "3  Afghanistan, Islamic Republic of           512   \n",
      "4  Afghanistan, Islamic Republic of           512   \n",
      "\n",
      "  Stocks, Transactions, and Other Flows Name  \\\n",
      "0    Holding gains and losses in liabilities   \n",
      "1    Holding gains and losses in liabilities   \n",
      "2    Holding gains and losses in liabilities   \n",
      "3    Holding gains and losses in liabilities   \n",
      "4    Holding gains and losses in liabilities   \n",
      "\n",
      "  Stocks, Transactions, and Other Flows Code                   Sector Name  \\\n",
      "0                                        G43  Budgetary central government   \n",
      "1                                        G43  Budgetary central government   \n",
      "2                                        G43  Budgetary central government   \n",
      "3                                        G43  Budgetary central government   \n",
      "4                                        G43  Budgetary central government   \n",
      "\n",
      "  Sector Code       Unit Name   Unit Code Residence Name Residence Code  ...  \\\n",
      "0      S1311B  Percent of GDP  XDC_R_B1GQ          Total          W0|S1  ...   \n",
      "1      S1311B  Percent of GDP  XDC_R_B1GQ          Total          W0|S1  ...   \n",
      "2      S1311B  Percent of GDP  XDC_R_B1GQ          Total          W0|S1  ...   \n",
      "3      S1311B  Percent of GDP  XDC_R_B1GQ          Total          W0|S1  ...   \n",
      "4      S1311B  Percent of GDP  XDC_R_B1GQ          Total          W0|S1  ...   \n",
      "\n",
      "  2009 2010 2011 2012 2013 2014 2015 Indicator Code  \\\n",
      "0  NaN  NaN  NaN  NaN   NP  NaN  NaN            NaN   \n",
      "1   NP   NP   NP   NP   NP   NP   NP            NaN   \n",
      "2   AC   AC   AC   AC   AC   AC   AC            NaN   \n",
      "3   NP   NP   NP   NP   NP   NP   NP            NaN   \n",
      "4  NaN  NaN  NaN  NaN    A  NaN  NaN            NaN   \n",
      "\n",
      "  Global DSD Time Series Code Unnamed: 59  \n",
      "0                         NaN         NaN  \n",
      "1                         NaN         NaN  \n",
      "2                         NaN         NaN  \n",
      "3                         NaN         NaN  \n",
      "4                         NaN         NaN  \n",
      "\n",
      "[5 rows x 60 columns]\n",
      "Headers saved\n"
     ]
    }
   ],
   "source": [
    "# Extract headers and save to CSV\n",
    "headers = pd.DataFrame(data.columns, columns=[\"Headers\"])\n",
    "headers.to_csv(\"headers.csv\", index=False)\n",
    "print(f\"Headers saved\")\n",
    "\n",
    "# Columns to extract unique values from\n",
    "columns_to_extract = [\"Residence Name\", \"Instrument and Assets Classification Name\"]\n",
    "\n",
    "# Extract and save unique values for each column\n",
    "for column in columns_to_extract:\n",
    "    unique_values = data[column].dropna().unique()  # Drop NaN values and get unique ones\n",
    "    unique_df = pd.DataFrame(unique_values, columns=[column])  # Convert to DataFrame\n",
    "    output_file = f\"unique_{column.replace(' ', '_')}.csv\"  # Save file\n",
    "    unique_df.to_csv(output_file, index=False)\n",
    "    print(f\"Unique values from '{column}' saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83409/63332209.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_rows['Keyword'] = keyword\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classification names: ['Holding gains and losses in liabilities' 'Net incurrence of liabilities'\n",
      " 'Stock position liabilities' 'Volume changes in liabilities'\n",
      " 'Other economic flows in liabilities']\n"
     ]
    }
   ],
   "source": [
    "def filter_and_combine(data, column_name, keywords, output_filename):\n",
    "    \"\"\"\n",
    "    Filters rows based on multiple keywords in a specified column and combines them into one CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame to search within.\n",
    "        column_name (str): The column to search for the keywords.\n",
    "        keywords (list): A list of keywords to search for.\n",
    "        output_filename (str): The filename to save the combined filtered rows.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of filtered rows.\n",
    "    \"\"\"\n",
    "    combined_rows = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Filter rows where the keyword appears in the specified column (case-insensitive)\n",
    "        filtered_rows = data[data[column_name].str.contains(keyword, case=False, na=False)]\n",
    "        \n",
    "        # Add a column to indicate the keyword used for filtering\n",
    "        filtered_rows['Keyword'] = keyword\n",
    "        \n",
    "        # Append to the combined DataFrame\n",
    "        combined_rows = pd.concat([combined_rows, filtered_rows], ignore_index=True)\n",
    "\n",
    "    # Save the combined rows to a single CSV file\n",
    "    # combined_rows.to_csv(Path.cwd() / output_filename, index=False)\n",
    "    \n",
    "    # Return the combined DataFrame\n",
    "    return combined_rows\n",
    "\n",
    "# Define the DataFrame (assuming `data` is already loaded)\n",
    "column_name = \"Stocks, Transactions, and Other Flows Name\"\n",
    "\n",
    "# List of keywords to search for\n",
    "keywords = [\"debt\", \"liabilities\", \"borrowing\"]\n",
    "\n",
    "# Call the function to filter and combine rows\n",
    "output_filename = \"filtered_rows_combined.csv\"\n",
    "combined_data = filter_and_combine(data, column_name, keywords, output_filename)\n",
    "\n",
    "# Display the unique values in the \"Classification Name\" column\n",
    "unique_classification_names = combined_data[column_name].unique()\n",
    "print(\"Unique classification names:\", unique_classification_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83409/1787307232.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_format(data):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to calculate the sum of legitimate entries and covered countries\n",
    "    for each combination of Classification, Sector, and Unit.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary DataFrame with analysis results.\n",
    "    \"\"\"\n",
    "    # Filter rows where Attribute is \"Value\"\n",
    "    filtered_data = data[data['Attribute'] == 'Value']\n",
    "\n",
    "    # Select year columns (assume all columns except metadata and Attribute are years)\n",
    "    year_columns = [col for col in data.columns if col.startswith('20') or col.startswith('19')]\n",
    "\n",
    "    # Group by Classification, Sector, and Unit\n",
    "    grouped = filtered_data.groupby(\n",
    "        ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', 'Residence Name', 'Instrument and Assets Classification Name']\n",
    "    )\n",
    "\n",
    "    # Calculate the required summaries\n",
    "    summary = grouped.apply(\n",
    "        lambda group: pd.Series({\n",
    "            'Sum of Legitimate Entries': group[year_columns].apply(\n",
    "                pd.to_numeric, errors='coerce'\n",
    "            ).count().sum(),\n",
    "            'Number of Covered Countries': group['Country Code'].nunique()\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Assuming your dataset is loaded in `data`\n",
    "summary = analyze_data_format(combined_data)\n",
    "\n",
    "# Save the summary to a CSV file if needed\n",
    "summary.to_csv(Path.cwd().parent.resolve()/\"data\"/\"GFSISB\"/\"Summaries\"/\"summary_analysis_2015.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108934/3284056054.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_rows['Keyword'] = keyword\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 2014 saved to /home/torbenhaferkamp/Desktop/IfW_Kiel/GSF/hidden_debt_gsf/src/hidden_debt_gsf/data/GFSISB/Summaries/summary_analysis_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108934/3284056054.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the data directory and output directory\n",
    "data_dir = Path.cwd().parent.resolve() / \"data\" / \"GFSISB\"\n",
    "output_dir = data_dir / \"Summaries\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# List of years to process\n",
    "# years = [2014, 2015, 2016, 2017, 2019, 2020]\n",
    "\n",
    "# List of years to process, to attain varnames\n",
    "years = [2014]\n",
    "\n",
    "# List of keywords for filtering\n",
    "keywords = [\"debt\", \"liabilities\", \"borrowing\"]\n",
    "\n",
    "# Chunksize for reading large files\n",
    "chunksize = 10_000\n",
    "\n",
    "def load_data(data_dir, year, chunksize):\n",
    "    \"\"\"Load the CSV file for a specific year in chunks.\"\"\"\n",
    "    file_dir = f\"GFSIBS{year}\"\n",
    "    file_name = f\"GFSIBS{year}_01-16-2025.csv\"\n",
    "    file_path = data_dir / file_dir / file_name\n",
    "    if file_path.exists():\n",
    "        data_chunks = pd.read_csv(file_path, chunksize=chunksize)\n",
    "        return pd.concat(data_chunks, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def filter_and_combine(data, column_name, keywords):\n",
    "    \"\"\"Filter rows based on multiple keywords and return combined results.\"\"\"\n",
    "    combined_rows = pd.DataFrame()\n",
    "    for keyword in keywords:\n",
    "        filtered_rows = data[data[column_name].str.contains(keyword, case=False, na=False)]\n",
    "        filtered_rows['Keyword'] = keyword\n",
    "        combined_rows = pd.concat([combined_rows, filtered_rows], ignore_index=True)\n",
    "    return combined_rows\n",
    "\n",
    "def analyze_data_format(data):\n",
    "    \"\"\"Analyze data to calculate sums of legitimate entries and covered countries.\"\"\"\n",
    "    filtered_data = data[data['Attribute'] == 'Value']\n",
    "    year_columns = [col for col in data.columns if col.startswith(('20', '19'))]\n",
    "    grouped = filtered_data.groupby(\n",
    "        ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', \n",
    "         'Residence Name', 'Instrument and Assets Classification Name']\n",
    "    )\n",
    "    summary = grouped.apply(\n",
    "        lambda group: pd.Series({\n",
    "            'Sum of Legitimate Entries': group[year_columns].apply(pd.to_numeric, errors='coerce').count().sum(),\n",
    "            'Number of Covered Countries': group['Country Code'].nunique()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    return summary\n",
    "\n",
    "def process_all_years(data_dir, years, keywords, output_dir, chunksize):\n",
    "    \"\"\"Process all years, filter and analyze data, and save summaries.\"\"\"\n",
    "    for year in years:\n",
    "        # Load data for the year\n",
    "        data = load_data(data_dir, year, chunksize)\n",
    "        if data is None:\n",
    "            continue  # Skip if file not found\n",
    "\n",
    "        # Filter and combine rows based on keywords\n",
    "        combined_data = filter_and_combine(data, \"Stocks, Transactions, and Other Flows Name\", keywords)\n",
    "\n",
    "        # Analyze data format\n",
    "        summary = analyze_data_format(combined_data)\n",
    "\n",
    "        # Save summary to a CSV file\n",
    "        summary_file = output_dir / f\"summary_analysis_{year}.csv\"\n",
    "        summary.to_csv(summary_file, index=False)\n",
    "        print(f\"Summary for {year} saved to {summary_file}\")\n",
    "\n",
    "# Execute the processing for all years\n",
    "process_all_years(data_dir, years, keywords, output_dir, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated summary saved to /home/torbenhaferkamp/Desktop/IfW_Kiel/GSF/hidden_debt_gsf/src/hidden_debt_gsf/data/GFSISB/Summaries/aggregated_summary_2014_2020.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory containing summary files\n",
    "summary_dir = output_dir\n",
    "\n",
    "# List of years to process\n",
    "years = [2014, 2015, 2016, 2017, 2019, 2020]\n",
    "\n",
    "# Load all summary files into a list of DataFrames\n",
    "summary_dfs = []\n",
    "for year in years:\n",
    "    file_path = summary_dir / f\"summary_analysis_{year}.csv\"\n",
    "    if file_path.exists():\n",
    "        summary_dfs.append(pd.read_csv(file_path))\n",
    "    else:\n",
    "        print(f\"Summary file for {year} not found at {file_path}\")\n",
    "\n",
    "# Combine all summaries into a single DataFrame\n",
    "combined_summary = pd.concat(summary_dfs, ignore_index=True)\n",
    "\n",
    "# Group by relevant columns and calculate aggregated values\n",
    "aggregated_summary = combined_summary.groupby(\n",
    "    # ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name'],\n",
    "    ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', 'Residence Name', 'Instrument and Assets Classification Name'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Sum of Legitimate Entries': 'sum',\n",
    "    'Number of Covered Countries': 'max'\n",
    "})\n",
    "\n",
    "# Sort by the most legitimate entries\n",
    "aggregated_summary = aggregated_summary.sort_values(by='Sum of Legitimate Entries', ascending=False)\n",
    "\n",
    "# Save the aggregated summary to a CSV file\n",
    "output_path = summary_dir / \"aggregated_summary_2014_2020.csv\"\n",
    "aggregated_summary.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Aggregated summary saved to {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidden_debt_gsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
