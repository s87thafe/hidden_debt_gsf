{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file\n",
    "sector_df = pd.read_csv('sector_S13_pivot.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 entries:\n",
      "      Country Code  Vintage  Year         Value    Difference  Percent_Change\n",
      "1725           158     2017  2008  1.204620e+13  1.249610e+13    -2777.528340\n",
      "921            134     2019  2014 -8.687000e+09 -9.022000e+09    -2693.134328\n",
      "569            128     2020  2018 -1.316100e+10 -1.389800e+10    -1885.753053\n",
      "1111           137     2017  2009 -1.230943e+09 -1.363210e+09    -1030.650476\n",
      "3145           233     2019  2015  2.334375e+13  2.605026e+13     -962.504542\n",
      "1715           158     2017  2006 -8.498100e+12 -1.081810e+13     -466.297414\n",
      "1494           144     2017  2012  2.026000e+09  2.591000e+09     -458.584071\n",
      "1527           146     2020  2009 -1.601105e+10 -2.112311e+10     -413.201346\n",
      "1720           158     2017  2007 -3.026300e+12 -4.429200e+12     -315.717442\n",
      "5708           936     2016  2004  1.749509e+09  2.769425e+09     -271.534557\n",
      "\n",
      "Last 10 entries:\n",
      "      Country Code  Vintage  Year         Value    Difference  Percent_Change\n",
      "5315           918     2019  2007  1.062700e+08  8.482000e+07      395.431235\n",
      "5667           935     2017  2013  2.508600e+10  2.047800e+10      444.401042\n",
      "1081           137     2017  2003  5.365840e+08  4.819009e+08      881.260988\n",
      "6365           960     2019  2007  7.933002e+09  7.267002e+09     1091.141441\n",
      "5718           936     2016  2006  6.283170e+08  6.030896e+08     2390.615519\n",
      "4882           826     2016  2013 -1.975082e+07 -1.921107e+07     3559.254234\n",
      "1846           172     2019  2017  1.033200e+10  1.033200e+10             inf\n",
      "2363           182     2019  2017  2.649050e+09  2.649050e+09             inf\n",
      "1903           174     2019  2017  9.139500e+08  9.139500e+08             inf\n",
      "1511           144     2019  2017  8.790000e+08  8.790000e+08             inf\n"
     ]
    }
   ],
   "source": [
    "# Sorting the DataFrame by the 'Percent_Change' column in ascending order\n",
    "sorted_df = sector_df.sort_values(by='Percent_Change')\n",
    "\n",
    "# Retrieving the first 10 entries\n",
    "first_10 = sorted_df.head(10)\n",
    "\n",
    "# Retrieving the last 10 entries\n",
    "last_10 = sorted_df.tail(10)\n",
    "\n",
    "# Displaying the results\n",
    "print(\"First 10 entries:\")\n",
    "print(first_10)\n",
    "\n",
    "print(\"\\nLast 10 entries:\")\n",
    "print(last_10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read the Parquet file\n",
    "merged_df = pd.read_parquet('filtered_merged_gsfibs.parquet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Country Name', 'Country Code',\n",
      "       'Stocks, Transactions, and Other Flows Name',\n",
      "       'Stocks, Transactions, and Other Flows Code', 'Sector Name',\n",
      "       'Sector Code', 'Unit Name', 'Unit Code', 'Residence Name',\n",
      "       'Residence Code', 'Instrument and Assets Classification Name',\n",
      "       'Instrument and Assets Classification Code', 'Attribute', '1972',\n",
      "       '1973', '1974', '1975', '1976', '1977', '1978', '1979', '1980', '1981',\n",
      "       '1982', '1983', '1984', '1985', '1986', '1987', '1988', '1989', '1990',\n",
      "       '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999',\n",
      "       '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008',\n",
      "       '2009', '2010', '2011', '2012', '2013', '2014', 'Indicator Code',\n",
      "       'Global DSD Time Series Code', 'Unnamed: 58', 'Vintage', '2015',\n",
      "       'Unnamed: 59', '2016', 'Unnamed: 60', '2017', 'Unnamed: 61', '2018',\n",
      "       '2019', 'Unnamed: 63', '2020', 'Unnamed: 64'],\n",
      "      dtype='object')\n",
      "['Net incurrence of liabilities' 'Holding gains and losses in liabilities'\n",
      " 'Stock position liabilities' 'Other economic flows in liabilities'\n",
      " 'Volume changes in liabilities']\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.columns)\n",
    "print(merged_df['Stocks, Transactions, and Other Flows Name'].unique())\n",
    "print(merged_df['Stocks, Transactions, and Other Flows Name'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector Name                                       Sector Code\n",
      "Budgetary central government                      S1311B         1039227\n",
      "Central government (excl. social security funds)  S1311          1024647\n",
      "Central government (incl. social security funds)  S1321          1010467\n",
      "Extrabudgetary central government                 S13112         1001841\n",
      "General government                                S13            1015848\n",
      "Local governments                                 S1313          1017028\n",
      "Social security funds                             S1314          1023204\n",
      "State governments                                 S1312           983147\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Group by the specified columns\n",
    "grouped_pairs = merged_df.groupby(['Sector Name','Sector Code']).size()\n",
    "\n",
    "# Print the grouped pairs\n",
    "print(grouped_pairs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df=merged_df[(merged_df['Attribute'] == 'Value') &\n",
    "            (merged_df['Unit Code'] == 'XDC')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All related data entries have been saved to 'missing_total_entries.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define the grouping columns, including yearly columns\n",
    "grouped_columns = [\n",
    "    'Country Code', 'Stocks, Transactions, and Other Flows Code', 'Sector Code',\n",
    "    'Instrument and Assets Classification Code', 'Vintage'\n",
    "]\n",
    "year_columns = list(map(str, range(1972, 2021)))\n",
    "\n",
    "# Split the data for Total and External Residence\n",
    "total_residence = filtered_df[filtered_df['Residence Code'] == 'W0|S1']\n",
    "external_residence = filtered_df[filtered_df['Residence Code'] == 'W1|S1']\n",
    "\n",
    "# Aggregate data for each residence group by grouping on relevant columns\n",
    "total_presence = total_residence.groupby(grouped_columns)[year_columns].any()\n",
    "external_presence = external_residence.groupby(grouped_columns)[year_columns].any()\n",
    "\n",
    "# Combine presence indicators to compare\n",
    "combined_presence = external_presence.join(total_presence, how='left', lsuffix='_external', rsuffix='_total')\n",
    "\n",
    "# Identify rows where External Residence data exists but Total Residence data does not\n",
    "missing_total = combined_presence[\n",
    "    (combined_presence.filter(regex='_external$').any(axis=1)) & \n",
    "    (~combined_presence.filter(regex='_total$').any(axis=1))\n",
    "].reset_index()\n",
    "\n",
    "# Extract rows from the filtered DataFrame that match missing_total groups\n",
    "missing_rows = filtered_df.merge(\n",
    "    missing_total[grouped_columns],\n",
    "    on=grouped_columns,\n",
    "    how='inner'\n",
    ")\n",
    "\n",
    "# Save the result to a CSV file\n",
    "missing_rows.to_csv('missing_total_entries.csv', index=False)\n",
    "# Save the result to a CSV file\n",
    "missing_total.to_csv('missing_total.csv', index=False)\n",
    "\n",
    "print(\"All related data entries have been saved to 'missing_total_entries.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sector reporting patterns and changes have been saved to CSV files.\n"
     ]
    }
   ],
   "source": [
    "# Define relevant columns\n",
    "relevant_columns = ['Country Code', 'Sector Name', 'Sector Code', 'Vintage', 'Instrument and Assets Classification Code', 'Residence Code', 'Stocks, Transactions, and Other Flows Code'] + list(map(str, range(1972, 2021)))\n",
    "\n",
    "# Filter the DataFrame for relevant sectors\n",
    "sector_data = filtered_df[filtered_df[relevant_columns]]\n",
    "\n",
    "# Group by country and sector\n",
    "grouped_sectors = sector_data.groupby(['Country Code', 'Sector Code', 'Vintage']).any()\n",
    "\n",
    "# Identify hierarchical reporting patterns\n",
    "# For example, check if 'S1311B' exists but not 'S13'\n",
    "hierarchical_patterns = grouped_sectors.reset_index()\n",
    "patterns = hierarchical_patterns.pivot_table(\n",
    "    index=['Country Code', 'Vintage'], \n",
    "    columns='Sector Code', \n",
    "    values=list(map(str, range(1972, 2021))),\n",
    "    aggfunc='any'\n",
    ")\n",
    "\n",
    "# Analyze patterns over time\n",
    "patterns_diff = patterns.diff(axis=1)\n",
    "\n",
    "# Save patterns to CSV for review\n",
    "patterns.to_csv('sector_reporting_patterns.csv')\n",
    "patterns_diff.to_csv('sector_reporting_changes.csv')\n",
    "\n",
    "print(\"Sector reporting patterns and changes have been saved to CSV files.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combination analysis and exclusivity violations have been saved to 'combination_analysis.csv' and 'violations.csv'.\n"
     ]
    }
   ],
   "source": [
    "# Define relevant columns for analysis\n",
    "relevant_columns = ['Stocks, Transactions, and Other Flows Code', \n",
    "                    'Sector Code', \n",
    "                    'Residence Code', \n",
    "                    'Instrument and Assets Classification Code']\n",
    "\n",
    "# Group by the relevant columns and summarize\n",
    "combination_analysis = (\n",
    "    filtered_df.groupby(relevant_columns)['Country Code']\n",
    "    .nunique()  # Count unique occurrences (e.g., countries or vintages)\n",
    "    .reset_index()\n",
    "    .rename(columns={'Country Code': 'Count'})\n",
    ")\n",
    "\n",
    "# Check exclusivity: identify combinations only linked to specific Sector Codes\n",
    "exclusive_combinations = combination_analysis.groupby([\n",
    "    'Stocks, Transactions, and Other Flows Code', \n",
    "    'Residence Code', \n",
    "    'Instrument and Assets Classification Code'\n",
    "])['Sector Code'].nunique().reset_index()\n",
    "\n",
    "# Filter for violations: cases where combinations occur with more than one Sector Code\n",
    "violations = exclusive_combinations[exclusive_combinations['Sector Code'] > 1]\n",
    "\n",
    "# Merge to get detailed information on violations\n",
    "detailed_violations = combination_analysis.merge(\n",
    "    violations, \n",
    "    on=['Stocks, Transactions, and Other Flows Code', \n",
    "        'Residence Code', \n",
    "        'Instrument and Assets Classification Code']\n",
    ")\n",
    "\n",
    "# Save the results to CSV files\n",
    "combination_analysis.to_csv('combination_analysis.csv', index=False)\n",
    "detailed_violations.to_csv('violations.csv', index=False)\n",
    "\n",
    "print(\"Combination analysis and exclusivity violations have been saved to 'combination_analysis.csv' and 'violations.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from src.hidden_debt_gsf.config import SRC, BLD_data\n",
    "\n",
    "\n",
    "def most_populated_combinations_gfsibs(\n",
    "        depends_on=BLD_data / \"Parquet\" / \"GFSIBS\" / \"filtered_merged_gsfibs.parquet\",\n",
    "        produces=BLD_data / \"DTA\" / \"GFSIBS\" / \"most_populated_float.dta\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Processes the filtered data to identify and extract the most populated \n",
    "    combination of Sector Code and Classification Code for each country.\n",
    "\n",
    "    Args:\n",
    "        depends_on (dict): Dictionary containing dependencies, in this case, \n",
    "                           the path to the filtered data.\n",
    "        produces (str): Path to the output .dta file.\n",
    "    \"\"\"\n",
    "    merged_df = pd.read_parquet(depends_on)\n",
    "\n",
    "    # Step 1: Initialize an empty list to store the most populated sector and classification data per country\n",
    "    most_populated_combinations = []\n",
    "\n",
    "    # Step 2: Get a list of unique country codes\n",
    "    unique_countries = merged_df['Country Code'].unique()\n",
    "\n",
    "    # Step 3: Iterate over each country and process its data\n",
    "    for country_code in unique_countries:\n",
    "        # Filter data for the current country\n",
    "        country_data = merged_df[\n",
    "            (merged_df['Country Code'] == country_code) & \n",
    "            (merged_df['Attribute'] == 'Value') & \n",
    "            (merged_df['Unit Code'] == 'XDC') & # Domestic Currency\n",
    "            (merged_df['Sector Code']!= 'S1312') & # State Governments\n",
    "            (merged_df['Sector Code']!= 'S1313') & # Local Governments\n",
    "            (merged_df['Sector Code']!= 'S1314') # Social security funds\n",
    "        ]\n",
    "\n",
    "        # Skip if no data for the country\n",
    "        if country_data.empty:\n",
    "            continue\n",
    "\n",
    "        # Identify year columns (1970 to 2020)\n",
    "        year_columns = [col for col in country_data.columns if col.isdigit() and 1970 <= int(col) <= 2020]\n",
    "\n",
    "        # Identify the most populated combination of Sector Code and Classification Code for the current country\n",
    "        most_data_entries = (\n",
    "            country_data\n",
    "            .groupby(['Sector Code', 'Classification Code'])[year_columns]\n",
    "            .apply(lambda group: group.notna().sum().sum())\n",
    "            .reset_index(name='Data Entry Count')\n",
    "            .sort_values(by='Data Entry Count', ascending=False)\n",
    "        )\n",
    "\n",
    "        # Get the most populated combination\n",
    "        if not most_data_entries.empty:\n",
    "            most_populated_combination = most_data_entries.iloc[0][['Sector Code', 'Classification Code']]\n",
    "            \n",
    "            # Filter data for the most populated combination\n",
    "            most_populated_combination_data = country_data[\n",
    "                (country_data['Sector Code'] == most_populated_combination['Sector Code']) &\n",
    "                (country_data['Classification Code'] == most_populated_combination['Classification Code'])\n",
    "            ]\n",
    "            \n",
    "            # Append the result to the list\n",
    "            most_populated_combinations.append(most_populated_combination_data)\n",
    "\n",
    "    # Step 4: Combine all the most populated combination data into a single DataFrame\n",
    "    combined_most_populated_combinations = pd.concat(most_populated_combinations, ignore_index=True)\n",
    "\n",
    "    # Save the combined data to a .dta file\n",
    "    combined_most_populated_combinations.to_stata(produces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (58,59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (56,57,58,59,60) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (59,60,61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (62,63) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n",
      "/tmp/ipykernel_125025/3832726373.py:15: DtypeWarning: Columns (61) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = pd.concat(data_chunks, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Get the data path\n",
    "data_path = Path.cwd().parent.resolve()/ \"src\" / \"hidden_debt_gsf\" /\"data\"/\"GFSISB\"/\"GFSIBS2020\"\n",
    "\n",
    "# Define the file name\n",
    "file_name = \"GFSIBS2020_01-16-2025.csv\"\n",
    "\n",
    "# Combine the base path and file name\n",
    "file_path = data_path / file_name\n",
    "\n",
    "# Read the file in chunks\n",
    "chunksize = 10_000\n",
    "\n",
    "if file_path.exists():  # Check if the file exists\n",
    "    data_chunks = pd.read_csv(file_path, chunksize=chunksize)\n",
    "    data = pd.concat(data_chunks, ignore_index=True)\n",
    "    print(\"File loaded successfully.\")\n",
    "else:\n",
    "    print(f\"File not found at {file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3374302\n",
      "488187\n",
      "64160\n",
      "32119\n",
      "14487\n",
      "1054\n",
      "160\n"
     ]
    }
   ],
   "source": [
    "print(len(data))\n",
    "var_of_interest = data[data['Stocks, Transactions, and Other Flows Name']=='Net incurrence of liabilities']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest = var_of_interest[var_of_interest['Sector Name']=='Budgetary central government']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest = var_of_interest[var_of_interest['Unit Name']=='Domestic currency']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest = var_of_interest[var_of_interest['Residence Name']=='Total']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest = var_of_interest[var_of_interest['Instrument and Assets Classification Name']=='Total financial assets/liabilities ']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest = var_of_interest[var_of_interest['Attribute']=='Value']\n",
    "print(len(var_of_interest))\n",
    "var_of_interest.to_csv('var_of_interest.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Headers saved\n",
      "Unique values from 'Residence Name' saved to unique_Residence_Name.csv\n",
      "Unique values from 'Instrument and Assets Classification Name' saved to unique_Instrument_and_Assets_Classification_Name.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract headers and save to CSV\n",
    "headers = pd.DataFrame(data.columns, columns=[\"Headers\"])\n",
    "headers.to_csv(\"headers.csv\", index=False)\n",
    "print(f\"Headers saved\")\n",
    "\n",
    "# Columns to extract unique values from\n",
    "columns_to_extract = [\"Residence Name\", \"Instrument and Assets Classification Name\"]\n",
    "\n",
    "# Extract and save unique values for each column\n",
    "for column in columns_to_extract:\n",
    "    unique_values = data[column].dropna().unique()  # Drop NaN values and get unique ones\n",
    "    unique_df = pd.DataFrame(unique_values, columns=[column])  # Convert to DataFrame\n",
    "    output_file = f\"unique_{column.replace(' ', '_')}.csv\"  # Save file\n",
    "    unique_df.to_csv(output_file, index=False)\n",
    "    print(f\"Unique values from '{column}' saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_83409/63332209.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_rows['Keyword'] = keyword\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique classification names: ['Holding gains and losses in liabilities' 'Net incurrence of liabilities'\n",
      " 'Stock position liabilities' 'Volume changes in liabilities'\n",
      " 'Other economic flows in liabilities']\n"
     ]
    }
   ],
   "source": [
    "def filter_and_combine(data, column_name, keywords, output_filename):\n",
    "    \"\"\"\n",
    "    Filters rows based on multiple keywords in a specified column and combines them into one CSV.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The DataFrame to search within.\n",
    "        column_name (str): The column to search for the keywords.\n",
    "        keywords (list): A list of keywords to search for.\n",
    "        output_filename (str): The filename to save the combined filtered rows.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Combined DataFrame of filtered rows.\n",
    "    \"\"\"\n",
    "    combined_rows = pd.DataFrame()  # Initialize an empty DataFrame\n",
    "\n",
    "    for keyword in keywords:\n",
    "        # Filter rows where the keyword appears in the specified column (case-insensitive)\n",
    "        filtered_rows = data[data[column_name].str.contains(keyword, case=False, na=False)]\n",
    "        \n",
    "        # Add a column to indicate the keyword used for filtering\n",
    "        filtered_rows['Keyword'] = keyword\n",
    "        \n",
    "        # Append to the combined DataFrame\n",
    "        combined_rows = pd.concat([combined_rows, filtered_rows], ignore_index=True)\n",
    "\n",
    "    # Save the combined rows to a single CSV file\n",
    "    # combined_rows.to_csv(Path.cwd() / output_filename, index=False)\n",
    "    \n",
    "    # Return the combined DataFrame\n",
    "    return combined_rows\n",
    "\n",
    "# Define the DataFrame (assuming `data` is already loaded)\n",
    "column_name = \"Stocks, Transactions, and Other Flows Name\"\n",
    "\n",
    "# List of keywords to search for\n",
    "keywords = [\"debt\", \"liabilities\", \"borrowing\"]\n",
    "\n",
    "# Call the function to filter and combine rows\n",
    "output_filename = \"filtered_rows_combined.csv\"\n",
    "combined_data = filter_and_combine(data, column_name, keywords, output_filename)\n",
    "\n",
    "# Display the unique values in the \"Classification Name\" column\n",
    "unique_classification_names = combined_data[column_name].unique()\n",
    "print(\"Unique classification names:\", unique_classification_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Stocks, Transactions, and Other Flows Name Sector Name Unit Name  \\\n",
      "0                                     Stocks    Sector B       EUR   \n",
      "1                               Transactions    Sector A       USD   \n",
      "\n",
      "  Residence Name Instrument and Assets Classification Name  \\\n",
      "0        Foreign                          Classification Y   \n",
      "1       Domestic                          Classification X   \n",
      "\n",
      "   Sum of Legitimate Entries  Number of Covered Countries  \n",
      "0                          4                            2  \n",
      "1                          4                            2  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114180/3917077512.py:24: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "def analyze_data_format(data):\n",
    "    \"\"\"\n",
    "    Analyzes the dataset to calculate the sum of legitimate entries and covered countries\n",
    "    for each combination of Classification, Sector, and Unit.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The input dataset.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Summary DataFrame with analysis results.\n",
    "    \"\"\"\n",
    "    # Filter rows where Attribute is \"Value\"\n",
    "    filtered_data = data[data['Attribute'] == 'Value']\n",
    "\n",
    "    # Select year columns (assume all columns except metadata and Attribute are years)\n",
    "    year_columns = [col for col in data.columns if col.startswith('20') or col.startswith('19')]\n",
    "\n",
    "    # Group by Classification, Sector, and Unit\n",
    "    grouped = filtered_data.groupby(\n",
    "        ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', 'Residence Name', 'Instrument and Assets Classification Name']\n",
    "    )\n",
    "\n",
    "    # Calculate the required summaries\n",
    "    summary = grouped.apply(\n",
    "        lambda group: pd.Series({\n",
    "            'Sum of Legitimate Entries': group[year_columns].apply(\n",
    "                pd.to_numeric, errors='coerce'\n",
    "            ).count().sum(),\n",
    "            'Number of Covered Countries': group['Country Code'].nunique()\n",
    "        })\n",
    "    ).reset_index()\n",
    "\n",
    "    return summary\n",
    "\n",
    "# Assuming your dataset is loaded in `data`\n",
    "summary = analyze_data_format(combined_data)\n",
    "\n",
    "# Save the summary to a CSV file if needed\n",
    "summary.to_csv(Path.cwd().parent.resolve()/\"data\"/\"GFSISB\"/\"Summaries\"/\"summary_analysis_2015.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108934/3284056054.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_rows['Keyword'] = keyword\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary for 2014 saved to /home/torbenhaferkamp/Desktop/IfW_Kiel/GSF/hidden_debt_gsf/src/hidden_debt_gsf/data/GFSISB/Summaries/summary_analysis_2014.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_108934/3284056054.py:51: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  summary = grouped.apply(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the data directory and output directory\n",
    "data_dir = Path.cwd().parent.resolve()/ \"src\" / \"hidden_debt_gsf\" / \"data\" / \"GFSISB\"\n",
    "output_dir = data_dir / \"Summaries\"\n",
    "\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# List of years to process\n",
    "# years = [2014, 2015, 2016, 2017, 2019, 2020]\n",
    "\n",
    "# List of years to process, to attain varnames\n",
    "years = [2014]\n",
    "\n",
    "# List of keywords for filtering\n",
    "keywords = [\"debt\", \"liabilities\", \"borrowing\"]\n",
    "\n",
    "# Chunksize for reading large files\n",
    "chunksize = 10_000\n",
    "\n",
    "def load_data(data_dir, year, chunksize):\n",
    "    \"\"\"Load the CSV file for a specific year in chunks.\"\"\"\n",
    "    file_dir = f\"GFSIBS{year}\"\n",
    "    file_name = f\"GFSIBS{year}_01-16-2025.csv\"\n",
    "    file_path = data_dir / file_dir / file_name\n",
    "    if file_path.exists():\n",
    "        data_chunks = pd.read_csv(file_path, chunksize=chunksize)\n",
    "        return pd.concat(data_chunks, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def filter_and_combine(data, column_name, keywords):\n",
    "    \"\"\"Filter rows based on multiple keywords and return combined results.\"\"\"\n",
    "    combined_rows = pd.DataFrame()\n",
    "    for keyword in keywords:\n",
    "        filtered_rows = data[data[column_name].str.contains(keyword, case=False, na=False)]\n",
    "        filtered_rows['Keyword'] = keyword\n",
    "        combined_rows = pd.concat([combined_rows, filtered_rows], ignore_index=True)\n",
    "    return combined_rows\n",
    "\n",
    "def analyze_data_format(data):\n",
    "    \"\"\"Analyze data to calculate sums of legitimate entries and covered countries.\"\"\"\n",
    "    filtered_data = data[data['Attribute'] == 'Value']\n",
    "    year_columns = [col for col in data.columns if col.startswith(('20', '19'))]\n",
    "    grouped = filtered_data.groupby(\n",
    "        ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', \n",
    "         'Residence Name', 'Instrument and Assets Classification Name']\n",
    "    )\n",
    "    summary = grouped.apply(\n",
    "        lambda group: pd.Series({\n",
    "            'Sum of Legitimate Entries': group[year_columns].apply(pd.to_numeric, errors='coerce').count().sum(),\n",
    "            'Number of Covered Countries': group['Country Code'].nunique()\n",
    "        })\n",
    "    ).reset_index()\n",
    "    return summary\n",
    "\n",
    "def process_all_years(data_dir, years, keywords, output_dir, chunksize):\n",
    "    \"\"\"Process all years, filter and analyze data, and save summaries.\"\"\"\n",
    "    for year in years:\n",
    "        # Load data for the year\n",
    "        data = load_data(data_dir, year, chunksize)\n",
    "        if data is None:\n",
    "            continue  # Skip if file not found\n",
    "\n",
    "        # Filter and combine rows based on keywords\n",
    "        combined_data = filter_and_combine(data, \"Stocks, Transactions, and Other Flows Name\", keywords)\n",
    "\n",
    "        # Analyze data format\n",
    "        summary = analyze_data_format(combined_data)\n",
    "\n",
    "        # Save summary to a CSV file\n",
    "        summary_file = output_dir / f\"summary_analysis_{year}.csv\"\n",
    "        summary.to_csv(summary_file, index=False)\n",
    "        print(f\"Summary for {year} saved to {summary_file}\")\n",
    "\n",
    "# Execute the processing for all years\n",
    "process_all_years(data_dir, years, keywords, output_dir, chunksize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aggregated summary saved to /home/torbenhaferkamp/Desktop/IfW_Kiel/GSF/hidden_debt_gsf/src/hidden_debt_gsf/data/GFSISB/Summaries/aggregated_summary_2014_2020.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the directory containing summary files\n",
    "summary_dir = output_dir\n",
    "\n",
    "# List of years to process\n",
    "years = [2014, 2015, 2016, 2017, 2019, 2020]\n",
    "\n",
    "# Load all summary files into a list of DataFrames\n",
    "summary_dfs = []\n",
    "for year in years:\n",
    "    file_path = summary_dir / f\"summary_analysis_{year}.csv\"\n",
    "    if file_path.exists():\n",
    "        summary_dfs.append(pd.read_csv(file_path))\n",
    "    else:\n",
    "        print(f\"Summary file for {year} not found at {file_path}\")\n",
    "\n",
    "# Combine all summaries into a single DataFrame\n",
    "combined_summary = pd.concat(summary_dfs, ignore_index=True)\n",
    "\n",
    "# Group by relevant columns and calculate aggregated values\n",
    "aggregated_summary = combined_summary.groupby(\n",
    "    # ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name'],\n",
    "    ['Stocks, Transactions, and Other Flows Name', 'Sector Name', 'Unit Name', 'Residence Name', 'Instrument and Assets Classification Name'],\n",
    "    as_index=False\n",
    ").agg({\n",
    "    'Sum of Legitimate Entries': 'sum',\n",
    "    'Number of Covered Countries': 'max'\n",
    "})\n",
    "\n",
    "# Sort by the most legitimate entries\n",
    "aggregated_summary = aggregated_summary.sort_values(by='Sum of Legitimate Entries', ascending=False)\n",
    "\n",
    "# Save the aggregated summary to a CSV file\n",
    "output_path = summary_dir / \"aggregated_summary_2014_2020.csv\"\n",
    "aggregated_summary.to_csv(output_path, index=False)\n",
    "\n",
    "print(f\"Aggregated summary saved to {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the data directory and output directory\n",
    "data_dir = Path.cwd().parent.resolve() / \"data\" / \"GFSISB\"\n",
    "output_dir = data_dir / \"Summaries\"\n",
    "output_dir.mkdir(parents=True, exist_ok=True)  # Ensure the output directory exists\n",
    "\n",
    "# List of years to process\n",
    "years = [2014, 2015, 2016, 2017, 2019, 2020]\n",
    "\n",
    "# Chunksize for reading large files\n",
    "chunksize = 5_000\n",
    "\n",
    "# Variables to process\n",
    "variables_to_process = [\n",
    "    {\n",
    "        \"Stocks, Transactions, and Other Flows Name\": \"Net incurrence of liabilities\",\n",
    "        \"Sector Name\": \"Budgetary central government\",\n",
    "        \"Unit Name\": \"Domestic currency\",\n",
    "        \"Residence Name\": \"External\",\n",
    "        \"Instrument and Assets Classification Name\": \"Total financial assets/liabilities \",\n",
    "    },\n",
    "    {\n",
    "        \"Stocks, Transactions, and Other Flows Name\": \"Net incurrence of liabilities\",\n",
    "        \"Sector Name\": \"Budgetary central government\",\n",
    "        \"Unit Name\": \"Domestic currency\",\n",
    "        \"Residence Name\": \"Total\",\n",
    "        \"Instrument and Assets Classification Name\": \"Total financial assets/liabilities \",\n",
    "    },\n",
    "    {\n",
    "        \"Stocks, Transactions, and Other Flows Name\": \"Net incurrence of liabilities\",\n",
    "        \"Sector Name\": \"Budgetary central government\",\n",
    "        \"Unit Name\": \"Domestic currency\",\n",
    "        \"Residence Name\": \"Domestic\",\n",
    "        \"Instrument and Assets Classification Name\": \"Total financial assets/liabilities \",\n",
    "    },\n",
    "]\n",
    "\n",
    "def load_data(data_dir, year, chunksize):\n",
    "    \"\"\"Load the CSV file for a specific year in chunks.\"\"\"\n",
    "    file_dir = f\"GFSIBS{year}\"\n",
    "    file_name = f\"GFSIBS{year}_01-16-2025.csv\"\n",
    "    file_path = data_dir / file_dir / file_name\n",
    "    if file_path.exists():\n",
    "        data_chunks = pd.read_csv(file_path, chunksize=chunksize)\n",
    "        return pd.concat(data_chunks, ignore_index=True)\n",
    "    else:\n",
    "        print(f\"File not found for year {year}: {file_path}\")\n",
    "        return None\n",
    "\n",
    "def calculate_numeric_counts(data, variables_to_process, year):\n",
    "    \"\"\"Calculate the number of numeric entries for each year column.\"\"\"\n",
    "    year_columns = [col for col in data.columns if col.startswith(('20', '19'))]\n",
    "    print(f\"Processing year {year} with year columns: {year_columns}\")\n",
    "    results = []\n",
    "\n",
    "    for variable in variables_to_process:\n",
    "        print(f\"Processing variable: {variable}\")\n",
    "        filtered_data = data.copy()\n",
    "\n",
    "        for col, value in variable.items():\n",
    "            print(f\"Filtering for column: {col} with value: {value}\")\n",
    "            if col not in filtered_data.columns:\n",
    "                print(f\"Column {col} not found in data!\")\n",
    "                continue\n",
    "            print(f\"Unique values in column {col}: {filtered_data[col].unique()}\")\n",
    "            filtered_data = filtered_data[filtered_data[col] == value]\n",
    "\n",
    "        if filtered_data.empty:\n",
    "            print(f\"No data found for variable: {variable} in year {year}\")\n",
    "            continue\n",
    "\n",
    "        # Count numeric entries in year columns\n",
    "        counts = filtered_data[year_columns].apply(pd.to_numeric, errors='coerce').count()\n",
    "        result = {\"Year\": year}\n",
    "        result.update(counts.to_dict())\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_variables_by_year(data_dir, years, variables_to_process, output_dir, chunksize):\n",
    "    \"\"\"Process variables by year and save results as individual DataFrames.\"\"\"\n",
    "    results_per_variable = {i: [] for i in range(len(variables_to_process))}\n",
    "\n",
    "    for year in years:\n",
    "        # Load data for the year\n",
    "        data = load_data(data_dir, year, chunksize)\n",
    "        if data is None:\n",
    "            continue  # Skip if file not found\n",
    "\n",
    "        # Calculate numeric counts for each variable\n",
    "        year_results = calculate_numeric_counts(data, variables_to_process, year)\n",
    "\n",
    "        # Append results for each variable\n",
    "        for i, result in enumerate(year_results):\n",
    "            results_per_variable[i].append(result)\n",
    "\n",
    "    # Save results for each variable as a DataFrame\n",
    "    for i, variable_results in results_per_variable.items():\n",
    "        if not variable_results:  # Skip if no results for a variable\n",
    "            print(f\"No data found for variable {i + 1}.\")\n",
    "            continue\n",
    "\n",
    "        df = pd.DataFrame(variable_results)\n",
    "        # Ensure the Year column exists\n",
    "        if \"Year\" not in df.columns:\n",
    "            df[\"Year\"] = []\n",
    "\n",
    "        # Reorder columns (Year first, then year columns in ascending order)\n",
    "        year_columns = sorted([col for col in df.columns if col != \"Year\"])\n",
    "        df = df[[\"Year\"] + year_columns]\n",
    "\n",
    "        # Save to CSV\n",
    "        output_file = output_dir / f\"variable_{i + 1}_numeric_counts.csv\"\n",
    "        df.to_csv(output_file, index=False)\n",
    "        print(f\"Results for variable {i + 1} saved to {output_file}\")\n",
    "\n",
    "# Execute the processing\n",
    "process_variables_by_year(data_dir, years, variables_to_process, output_dir, chunksize)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hidden_debt_gsf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
